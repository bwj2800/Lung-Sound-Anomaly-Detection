{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ebea4f-ce93-4fc6-bbbd-79070f50124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 10:41:28.468618: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-25 10:41:29.024548: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 10:41:29.816496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import peak_widths\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse.csgraph import min_weight_full_bipartite_matching\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.optimize import minimize\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdea43a-caf8-4712-8fbf-a6615bd33a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bc368-146d-40ac-b20c-5a9c26af880c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "folds_file = './ICBHI_Dataset/patient_list_foldwise.txt'\n",
    "# train_flag = train_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d87564-4b55-4f2d-a6ea-4bdeff15dcdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/monetai2/Desktop/LabFolder/Kim/LungSound/V5_mspec_8s_4gr/ICBHI_Dataset/audio_and_txt_files/'\n",
    "# file_name = './Dataset/audio_and_txt_files/'\n",
    "def Extract_Annotation_Data(file_name, data_dir):\n",
    "\ttokens = file_name.split('_')\n",
    "\trecording_info = pd.DataFrame(data = [tokens], columns = ['Patient Number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n",
    "\trecording_annotations = pd.read_csv(os.path.join(data_dir, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n",
    "\treturn recording_info, recording_annotations\n",
    "\n",
    "# get annotations data and filenames\n",
    "def get_annotations(data_dir):\n",
    "\tfilenames = [s.split('.')[0] for s in os.listdir(data_dir) if '.txt' in s]\n",
    "\ti_list = []\n",
    "\trec_annotations_dict = {}\n",
    "\tfor s in filenames:\n",
    "\t\ti,a = Extract_Annotation_Data(s, data_dir)\n",
    "\t\ti_list.append(i)\n",
    "\t\trec_annotations_dict[s] = a\n",
    "\n",
    "\trecording_info = pd.concat(i_list, axis = 0)\n",
    "\trecording_info.head()\n",
    "\n",
    "\treturn filenames, rec_annotations_dict\n",
    "\n",
    "def slice_data(start, end, raw_data, sample_rate):\n",
    "\tmax_ind = len(raw_data) \n",
    "\tstart_ind = min(int(start * sample_rate), max_ind)\n",
    "\tend_ind = min(int(end * sample_rate), max_ind)\n",
    "\treturn raw_data[start_ind: end_ind]\n",
    "\n",
    "# def get_label(crackle, wheeze):\n",
    "# \tif crackle == 0 and wheeze == 0:\n",
    "# \t\treturn 0\n",
    "# \telse:\n",
    "# \t\treturn 1\n",
    "def get_label(crackle, wheeze):\n",
    "\tif crackle == 0 and wheeze == 0:\n",
    "\t\treturn 0\n",
    "\telif crackle == 1 and wheeze == 0:\n",
    "\t\treturn 1\n",
    "\telif crackle == 0 and wheeze == 1:\n",
    "\t\treturn 2\n",
    "\telse:\n",
    "\t\treturn 3\n",
    "\n",
    "def get_sound_samples(recording_annotations, file_name, data_dir, sample_rate):\n",
    "\tsample_data = [file_name]\n",
    "\t# load file with specified sample rate (also converts to mono)\n",
    "\tdata, rate = librosa.load(os.path.join(data_dir, file_name+'.wav'), sr=sample_rate)\n",
    "\t#print(\"Sample Rate\", rate)\n",
    "\t\n",
    "\tfor i in range(len(recording_annotations.index)):\n",
    "\t\trow = recording_annotations.loc[i]\n",
    "\t\tstart = row['Start']\n",
    "\t\tend = row['End']\n",
    "\t\tcrackles = row['Crackles']\n",
    "\t\twheezes = row['Wheezes']\n",
    "\t\taudio_chunk = slice_data(start, end, data, rate)\n",
    "\t\tsample_data.append((audio_chunk, start,end, get_label(crackles, wheezes)))\n",
    "\treturn sample_data\n",
    "filenames, rec_annotations_dict = get_annotations(data_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d2d27-0085-47e4-b341-15480b4e2015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_rate = 4000\n",
    "filenames_with_labels = []\n",
    "print(\"Exracting Individual Cycles\")\n",
    "cycle_list = []\n",
    "# classwise_cycle_list = [[], []] #2-class\n",
    "classwise_cycle_list = [[], [], [], []] #4-class\n",
    "for idx, file_name in tqdm(enumerate(filenames)):\n",
    "    data = get_sound_samples(rec_annotations_dict[file_name], file_name, data_dir, sample_rate)\n",
    "    # print('--------', data)\n",
    "    cycles_with_labels = [(d[0], d[3], file_name, cycle_idx, d[3]) for cycle_idx, d in enumerate(data[1:])] #lable: d[3]\n",
    "    # cycles_with_labels = [(d[0], d[3]) for cycle_idx, d in enumerate(data[1:])] #lable: d[3]\n",
    "    # print('cycles_with_labels: ', cycles_with_labels)\n",
    "    cycle_list.extend(cycles_with_labels)\n",
    "    for cycle_idx, d in enumerate(cycles_with_labels):\n",
    "        filenames_with_labels.append(file_name+'_'+str(d[3])+'_'+str(d[1]))\n",
    "        classwise_cycle_list[d[1]].append(d)\n",
    "print(len(cycle_list))\n",
    "print(len(classwise_cycle_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment normal\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)\n",
    "scale = 1\n",
    "aug_nos = scale*len(classwise_cycle_list[0]) - len(classwise_cycle_list[0])\n",
    "for idx in range(aug_nos):\n",
    "    # normal_i + normal_j\n",
    "    i = random.randint(0, len(classwise_cycle_list[0])-1)\n",
    "    j = random.randint(0, len(classwise_cycle_list[0])-1)\n",
    "    normal_i = classwise_cycle_list[0][i]\n",
    "    normal_j = classwise_cycle_list[0][j]\n",
    "    new_sample = np.concatenate([normal_i[0], normal_j[0]])\n",
    "    cycle_list.append((new_sample, 0, normal_i[2]+'-'+normal_j[2], idx, 0))\n",
    "    filenames_with_labels.append(normal_i[2]+'-'+normal_j[2]+'_'+str(idx)+'_0')\n",
    "    \n",
    "# augment abnormal\n",
    "aug_nos = scale*len(classwise_cycle_list[0]) - len(classwise_cycle_list[1])\n",
    "for idx in range(aug_nos):\n",
    "    aug_prob = random.random()\n",
    "    if aug_prob < 0.6:\n",
    "        # crackle_i + crackle_j\n",
    "        i = random.randint(0, len(classwise_cycle_list[1])-1)\n",
    "        j = random.randint(0, len(classwise_cycle_list[1])-1)\n",
    "        sample_i = classwise_cycle_list[1][i]\n",
    "        sample_j = classwise_cycle_list[1][j]\n",
    "    elif aug_prob >= 0.6 and aug_prob < 0.8:\n",
    "        # crackle_i + normal_j\n",
    "        i = random.randint(0, len(classwise_cycle_list[1])-1)\n",
    "        j = random.randint(0, len(classwise_cycle_list[0])-1)\n",
    "        sample_i = classwise_cycle_list[1][i]\n",
    "        sample_j = classwise_cycle_list[0][j]\n",
    "    else:\n",
    "        # normal_i + crackle_j\n",
    "        i = random.randint(0, len(classwise_cycle_list[0])-1)\n",
    "        j = random.randint(0, len(classwise_cycle_list[1])-1)\n",
    "        sample_i = classwise_cycle_list[0][i]\n",
    "        sample_j = classwise_cycle_list[1][j]\n",
    "\n",
    "    new_sample = np.concatenate([sample_i[0], sample_j[0]])\n",
    "    cycle_list.append((new_sample, 1, sample_i[2]+'-'+sample_j[2], idx, 0))\n",
    "    filenames_with_labels.append(sample_i[2]+'-'+sample_j[2]+'_'+str(idx)+'_1')\n",
    "print(len(cycle_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cycle_list[1:])\n",
    "# print(cycle_list[0:10])\n",
    "# for c in enumerate(cycle_list[1:10]):\n",
    "#     print(c[1])\n",
    "#     print(c[1][1])\n",
    "\n",
    "cycle_list_new_data = []\n",
    "cycles_with_labels_new = [(c[1][0], c[1][1]) for c in enumerate(cycle_list[0:])]\n",
    "cycle_list_new_data.extend(cycles_with_labels_new)\n",
    "print(len(cycle_list_new_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d355161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the desired sequence length\n",
    "max_sequence_length = 3*sample_rate\n",
    "\n",
    "# Define your dense layer for data compression\n",
    "dense_layer = tf.keras.layers.Dense(units=max_sequence_length, activation='relu')\n",
    "\n",
    "# Separate data and labels\n",
    "X = np.array([item[0] for item in cycle_list_new_data], dtype=object)\n",
    "y = np.array([item[1] for item in cycle_list_new_data])\n",
    "\n",
    "# Initialize X_new list to store modified sequences\n",
    "X_new = []\n",
    "\n",
    "for x in X:\n",
    "    if len(x) > max_sequence_length:\n",
    "        t = max_sequence_length // len(x)\n",
    "        if max_sequence_length % len(x) == 0:\n",
    "            repeat_x = np.tile(x, t)\n",
    "            copy_repeat_x = repeat_x.copy()\n",
    "            X_new.append(copy_repeat_x)\n",
    "        else:\n",
    "            d = max_sequence_length % len(x)\n",
    "            # print('ddddd', d)\n",
    "            d = x[:d]\n",
    "            # print('dddddddd: ', d)\n",
    "            # print('soundclip*t:', len(np.tile(soundclip, t)), n_samples*t)\n",
    "            repeat_x = np.concatenate((np.tile(x, t), d))\n",
    "            copy_repeat_x = repeat_x.copy()\n",
    "            # print('copy_repeat_sample:', len(copy_repeat_sample))\n",
    "            X_new.append(copy_repeat_x)\n",
    "    else:\n",
    "        padded_sequence = np.pad(x, (0, max_sequence_length - len(x)), 'constant')\n",
    "        X_new.append(padded_sequence)\n",
    "        # print('Smaller:', padded_sequence.shape)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_new = np.array(X_new)\n",
    "y = np.array(y)\n",
    "X = X_new\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee67be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling2D, Flatten, Dense, Concatenate, GlobalAveragePooling1D, Activation, Add, AveragePooling1D, ReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import binary_accuracy, Precision, Recall\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def residual_block(x, filters, dilation_rate):\n",
    "    # Define the residual block\n",
    "    res = Conv1D(filters=filters, kernel_size=3, dilation_rate=dilation_rate, padding='same')(x)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Activation('relu')(res)\n",
    "    res = Conv1D(filters=filters, kernel_size=3, padding='same')(res)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Add()([x, res])\n",
    "    res = Activation('relu')(res)\n",
    "    return res\n",
    "\n",
    "feature_size = max_sequence_length\n",
    "# Define the input shape\n",
    "input_shape = (feature_size, 1) \n",
    "\n",
    "# Define the input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "x = Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = AveragePooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.15)(x)\n",
    "x = AveragePooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(256, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(256, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = AveragePooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(512, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Conv1D(512, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "\n",
    "# Define the TCN layers\n",
    "dilation_bases = [2, 3]\n",
    "# dilation_factors = [1, 2, 4]\n",
    "tcn_outputs = []\n",
    "for dilation_base in dilation_bases:\n",
    "    for i in range(3):\n",
    "        dilation_rate = dilation_base**i\n",
    "        x = residual_block(x, filters=512, dilation_rate=dilation_rate)\n",
    "    tcn_outputs.append(x)\n",
    "\n",
    "# Define the fusion layer\n",
    "fusion = Concatenate(axis=-1)(tcn_outputs)\n",
    "fusion = GlobalAveragePooling1D()(fusion)\n",
    "\n",
    "# Define the classifier\n",
    "x = Dense(units=512, activation='relu')(fusion)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(units=128, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(units=32, activation='relu')(x)\n",
    "outputs = Dense(units=4, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), \n",
    "                       tf.keras.metrics.Recall()]) #[binary_accuracy, Precision(), Recall()]\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387337f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess the image data\n",
    "best_test_acc = 0.0\n",
    "best_model = None\n",
    "sp_scores, se_scores, sc_scores = [], [], []\n",
    "ppv, se_class, f1, sp_class, acc = [], [], [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True,test_size=0.4, random_state=42, stratify=y)\n",
    "    \n",
    "    unique, frequency = np.unique(y_test, return_counts = True)\n",
    "    print([\"{:0.3f}\".format(fre/len(y_test)) for fre in frequency])\n",
    "    # Preprocess the image data\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_test = preprocess_input(X_test)\n",
    "    y_train = y_train.ravel()\n",
    "    y_test = y_test.ravel()\n",
    "    y_train = to_categorical(y_train, num_classes=4)\n",
    "    y_test = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "    # model = build_model(input_shape)\n",
    "    # model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy', tf.keras.metrics.Precision(), \n",
    "                        tf.keras.metrics.Recall()]) \n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), callbacks=[callback])\n",
    "    test_metrics = model.evaluate(X_test, y_test)\n",
    "    test_acc = test_metrics[1]\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_model = model\n",
    "    best_model.save(f'./model_seed/LN_ML_TCN/60_40/Sequence/Dup_Trun/model_Aug_{i}.h5')\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "    y_test = np.argmax(y_test, axis = 1)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    # clas_report = classification_report(y_val, y_pred)\n",
    "    avg_cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    se = (avg_cm[1][1] + avg_cm[2][2] + avg_cm[3][3])/(avg_cm[1][0] + avg_cm[1][1] + avg_cm[1][2] + avg_cm[1][3] \n",
    "                                                    + avg_cm[2][0] + avg_cm[2][1] + avg_cm[2][2] + avg_cm[2][3]\n",
    "                                                    + avg_cm[3][0] + avg_cm[3][1] + avg_cm[3][2] + avg_cm[3][3])\n",
    "    sp = avg_cm[0][0]/(avg_cm[0][0] + avg_cm[0][1] + avg_cm[0][2] + avg_cm[0][3])\n",
    "    sc = (se+sp)/2\n",
    "    sp_scores.append(sp)\n",
    "    se_scores.append(se)\n",
    "    sc_scores.append(sc)\n",
    "    # print(f\"Fold {i+1}: Sp={sp:.4f}, Se={se:.4f}, Sc={sc:.4f}\")\n",
    "    # Reset the model for the next fold\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Caclulate the value of each class\n",
    "    # confusion_matrix = avg_cm\n",
    "    num_classes = avg_cm.shape[0]\n",
    "\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1_sc = np.zeros(num_classes)\n",
    "    accuracy = np.zeros(num_classes)\n",
    "    specificity = np.zeros(num_classes)\n",
    "\n",
    "    for j in range(num_classes):\n",
    "        true_positive = avg_cm[j, j]\n",
    "        false_positive = np.sum(avg_cm[:, j]) - true_positive\n",
    "        false_negative = np.sum(avg_cm[j, :]) - true_positive\n",
    "        true_negative = np.sum(avg_cm) - (true_positive + false_positive + false_negative)\n",
    "\n",
    "        accuracy[j] = (true_positive + true_negative) / np.sum(avg_cm)\n",
    "        specificity[j] = true_negative / (true_negative + false_positive)\n",
    "\n",
    "        precision[j] = true_positive / (true_positive + false_positive)\n",
    "        recall[j] = true_positive / (true_positive + false_negative)\n",
    "        f1_sc[j] = 2 * (precision[j] * recall[j]) / (precision[j] + recall[j])\n",
    "\n",
    "    ppv.append(precision)\n",
    "    se_class.append(recall)\n",
    "    f1.append(f1_sc)\n",
    "    sp_class.append(specificity)\n",
    "    acc.append(accuracy)\n",
    "\n",
    "# Print overall scores\n",
    "print(\"\\nAverage scores:\")\n",
    "print(f\"Sp: {np.mean(sp_scores):.4f}\")\n",
    "print(f\"Se: {np.mean(se_scores):.4f}\")\n",
    "print(f\"Sc: {np.mean(sc_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5da407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results in a .txt file\n",
    "with open('./model_seed/LN_ML_TCN/60_40/Sequence/Dup_Trun/Mean_results_Aug.txt', 'w') as file:\n",
    "    file.write('Specificity Sp: {}\\n'.format(np.mean(sp_scores)))\n",
    "    file.write('Sensitivity Se: {}\\n'.format(np.mean(se_scores)))\n",
    "    file.write('Score Sc: {}\\n'.format(np.mean(sc_scores)))\n",
    "    file.write('confusion_matrix for each class: {}\\n'.format(confusion_matrix))\n",
    "\n",
    "# Save the results in a .txt file\n",
    "with open('./model_seed/LN_ML_TCN/60_40/Sequence/Dup_Trun/Each_fold_results_Aug.txt', 'w') as file:\n",
    "    file.write('Specificity Sp: {}\\n'.format(sp_scores))\n",
    "    file.write('Sensitivity Se: {}\\n'.format(se_scores))\n",
    "    file.write('Score Sc: {}\\n'.format(sc_scores))\n",
    "\n",
    "# Save the results in a .txt file\n",
    "with open('./model_seed/LN_ML_TCN/60_40/Sequence/Dup_Trun/Each_class_results_Aug.txt', 'w') as file:\n",
    "    file.write('PPv: {}\\n'.format(ppv))\n",
    "    file.write('Se: {}\\n'.format(se_class))\n",
    "    file.write('F1: {}\\n'.format(f1))\n",
    "    file.write('Sp_class: {}\\n'.format(sp_class))\n",
    "    file.write('Acc: {}\\n'.format(acc))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
