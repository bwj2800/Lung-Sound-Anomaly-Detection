{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "import pywt\n",
    "from skimage.feature import greycomatrix\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import metricspo\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "kl = keras.layers\n",
    "import skimage\n",
    "import random\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "# tf.keras.utils.set_random_seed(seed_value)\n",
    "# tf.random.set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1651042938509,
     "user": {
      "displayName": "Kim-Ngoc Thi Le",
      "userId": "07740631987100063646"
     },
     "user_tz": -540
    },
    "id": "WIt-2ocpPoWs"
   },
   "outputs": [],
   "source": [
    "def im2double(img):\n",
    "    \"\"\" convert image to double format \"\"\"\n",
    "    min_val = np.min(img.ravel())\n",
    "    max_val = np.max(img.ravel())\n",
    "    out = (img.astype('float') - min_val) / (max_val - min_val)\n",
    "    return out\n",
    "# =============================================================================\n",
    "# compute_14_features\n",
    "# =============================================================================\n",
    "def compute_14_features(region):\n",
    "    \"\"\" Compute 14 features \"\"\"\n",
    "    temp_array=region.reshape(-1)\n",
    "    all_pixels=temp_array[temp_array!=0]\n",
    "#    Area\n",
    "    Area = np.sum(all_pixels)\n",
    "#    mean\n",
    "    density = np.mean(all_pixels)\n",
    "#   Std\n",
    "    std_Density = np.std(all_pixels)\n",
    "#   skewness\n",
    "    Skewness = skew(all_pixels)\n",
    "#   kurtosis\n",
    "    Kurtosis = kurtosis(all_pixels)\n",
    "#   Energy\n",
    "    ENERGY =np.sum(np.square(all_pixels))\n",
    "#   Entropy\n",
    "    value,counts = np.unique(all_pixels, return_counts=True)\n",
    "    p = counts / np.sum(counts)\n",
    "    p =  p[p!=0]\n",
    "    ENTROPY =-np.sum( p*np.log2(p));\n",
    "#   Maximum\n",
    "    MAX = np.max(all_pixels)\n",
    "#   Mean Absolute Deviation\n",
    "    sum_deviation= np.sum(np.abs(all_pixels-np.mean(all_pixels)))\n",
    "    mean_absolute_deviation = sum_deviation/len(all_pixels)\n",
    "#   Median\n",
    "    MEDIAN = np.median(all_pixels)\n",
    "#   Minimum\n",
    "    MIN = np.min(all_pixels)\n",
    "#   Range\n",
    "    RANGE = np.max(all_pixels)-np.min(all_pixels)\n",
    "#   Root Mean Square\n",
    "    RMS = np.sqrt(np.mean(np.square(all_pixels))) \n",
    "#    Uniformity\n",
    "    UNIFORMITY = np.sum(np.square(p))\n",
    "\n",
    "    features = np.array([Area, density, std_Density,\n",
    "        Skewness, Kurtosis,ENERGY, ENTROPY,\n",
    "        MAX, mean_absolute_deviation, MEDIAN, MIN, RANGE, RMS, UNIFORMITY])\n",
    "    return features\n",
    "# =============================================================================\n",
    "# GLDM\n",
    "# =============================================================================\n",
    "def GLDM(img, distance):\n",
    "    \"\"\" GLDM in four directions \"\"\"\n",
    "    pro1=np.zeros(img.shape,dtype=np.float32)\n",
    "    pro2=np.zeros(img.shape,dtype=np.float32)\n",
    "    pro3=np.zeros(img.shape,dtype=np.float32)\n",
    "    pro4=np.zeros(img.shape,dtype=np.float32)\n",
    "    \n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "\n",
    "            if((j+distance)<img.shape[1]):\n",
    "                pro1[i,j]=np.abs(img[i,j]-img[i,(j+distance)])\n",
    "            if((i-distance)>0)&((j+distance)<img.shape[1]):\n",
    "                pro2[i,j]=np.abs(img[i,j]-img[(i-distance),(j+distance)])\n",
    "            if((i+distance)<img.shape[0]):\n",
    "                pro3[i,j]=np.abs(img[i,j]-img[(i+distance),j])\n",
    "            if((i-distance)>0)&((j-distance)>0):\n",
    "                pro4[i,j]=np.abs(img[i,j]-img[(i-distance),(j-distance)])\n",
    "\n",
    "    n=256;\n",
    "    cnt, bin_edges=np.histogram(pro1[pro1!=0], bins=np.arange(n)/(n-1), density=False)\n",
    "    Out1 = cnt.cumsum()\n",
    "    cnt, bin_edges=np.histogram(pro2[pro2!=0], bins=np.arange(n)/(n-1), density=False)\n",
    "    Out2 = cnt.cumsum()\n",
    "    cnt, bin_edges=np.histogram(pro3[pro3!=0], bins=np.arange(n)/(n-1), density=False)\n",
    "    Out3 = cnt.cumsum()\n",
    "    cnt, bin_edges=np.histogram(pro4[pro4!=0], bins=np.arange(n)/(n-1), density=False)\n",
    "    Out4 = cnt.cumsum()\n",
    "    return Out1,Out2,Out3,Out4\n",
    "# =============================================================================\n",
    "#   show model\n",
    "# =============================================================================\n",
    "def show_model(model):\n",
    "    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "    return Image(filename='model.png')\n",
    "\n",
    "# =============================================================================\n",
    "# plot confusion matrix\n",
    "# =============================================================================\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89489,
     "status": "ok",
     "timestamp": 1651043074005,
     "user": {
      "displayName": "Kim-Ngoc Thi Le",
      "userId": "07740631987100063646"
     },
     "user_tz": -540
    },
    "id": "nBhqAj8uPzOZ",
    "outputId": "012a57b7-3bec-421a-acc4-ddebf89d36e7"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# resizing, normalization and adaptive histogram equalization to images\n",
    "# =============================================================================\n",
    "import skimage.io as io\n",
    "from skimage.transform import  rescale,resize\n",
    "from skimage.util import img_as_uint,img_as_ubyte\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure\n",
    "import os\n",
    "import numpy as np\n",
    "# =============================================================================\n",
    "# source and destination dirs\n",
    "# =============================================================================\n",
    "class_name='wheeze'# crackle, both, normal\n",
    "source_dir='./data_4gr//original_images//'+class_name\n",
    "destination_dir='./data_4gr//original_images_preprocessed//'+class_name\n",
    "\n",
    "# =============================================================================\n",
    "# list images list from source dir\n",
    "# =============================================================================\n",
    "image_list=os.listdir(source_dir)# list of images\n",
    "# =============================================================================\n",
    "# Normalization and adaptive histogram equalization to each single image\n",
    "# =============================================================================\n",
    "import imageio\n",
    "from PIL import Image\n",
    "for img_name in image_list:\n",
    "    img=io.imread(os.path.join(source_dir,img_name), as_gray=False)\n",
    "    # img = imageio.imread(img)[:,:,:3]\n",
    "    # print(img.shape)\n",
    "    # print('len', len(img.shape))\n",
    "    if len(img.shape) ==3:\n",
    "        if img.shape[-1] == 4:\n",
    "            # print('anh dau', img)\n",
    "            img = img[:,:,:3]\n",
    "            # print('chuyen4 to 3', img.shape)\n",
    "            img_gray = rgb2gray(img)\n",
    "        else:\n",
    "            img_gray = rgb2gray(img)\n",
    "    else:\n",
    "        img_gray = img\n",
    "    img_resized = resize(img_gray, (512, 512))#convert image size to 512*512\n",
    "    img_rescaled=(img_resized-np.min(img_resized))/(np.max(img_resized)-np.min(img_resized))#min-max normalization \n",
    "    img_enhanced=exposure.equalize_adapthist(img_rescaled)#adapt hist\n",
    "    img_resized_8bit=img_as_ubyte(img_enhanced)\n",
    "    io.imsave(os.path.join(destination_dir,img_name),img_resized_8bit)#save enhanced image to destination dir   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5UNABvPieF6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# feature extraction to create feature pool\n",
    "# =============================================================================\n",
    "import skimage.io as io\n",
    "from skimage.transform import  rescale,resize\n",
    "from skimage.util import img_as_uint,img_as_ubyte\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import*\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# =============================================================================\n",
    "# source dir and output file name\n",
    "# =============================================================================\n",
    "class_name='normal'#crackle, both, normal\n",
    "source_dir='./data_4gr//original_images_preprocessed//'+class_name\n",
    "output_file_name=class_name\n",
    "# =============================================================================\n",
    "# set labels\n",
    "# =============================================================================\n",
    "if output_file_name=='normal':\n",
    "    label=0\n",
    "elif output_file_name=='crackle':\n",
    "    label=1\n",
    "elif output_file_name=='wheeze':\n",
    "    label=2    \n",
    "else:\n",
    "    label=3\n",
    "# =============================================================================\n",
    "# start\n",
    "# =============================================================================\n",
    "image_list=os.listdir(source_dir)#list of images\n",
    "\n",
    "feature_pool=np.empty([1,322])#feature pool [1,252]\n",
    "for idx,img_name in enumerate(image_list):\n",
    "    # print(idx)\n",
    "    img=io.imread(os.path.join(source_dir,img_name))\n",
    "    img_rescaled=(img-np.min(img))/(np.max(img)-np.min(img)) \n",
    "    # print(img.shape)\n",
    "    texture_features=compute_14_features(img_rescaled)#texture features\n",
    "    \n",
    "    fft_map=np.fft.fft2(img_rescaled)\n",
    "    fft_map = np.fft.fftshift(fft_map)\n",
    "    fft_map = np.abs(fft_map)\n",
    "    YC=int(np.floor(fft_map.shape[1]/2)+1)\n",
    "    fft_map=fft_map[:,YC:int(np.floor(3*YC/2))]\n",
    "    # print('fft_map: ', fft_map.shape)\n",
    "    fft_features=compute_14_features(fft_map)#FFT features\n",
    "    \n",
    "    wavelet_coeffs = pywt.dwt2(img_rescaled,'sym4')\n",
    "    cA1, (cH1, cV1, cD1) = wavelet_coeffs\n",
    "    # print('cA1: ', cA1.shape)\n",
    "    # print('cH1: ', cH1.shape)\n",
    "    wavelet_coeffs = pywt.dwt2(cA1,'sym4')\n",
    "    cA2, (cH2, cV2, cD2) = wavelet_coeffs#wavelet features\n",
    "    wavelet_features=np.concatenate((compute_14_features(cA1), compute_14_features(cH1),compute_14_features(cV1),compute_14_features(cD1)\n",
    "    ,compute_14_features(cA2), compute_14_features(cH2),compute_14_features(cV2),compute_14_features(cD2)), axis=0)\n",
    "    \n",
    "    \n",
    "    gLDM1,gLDM2,gLDM3,gLDM4=GLDM(img_rescaled,10)#GLDM in four directions\n",
    "    # print('gLDM1: ', gLDM1.shape)\n",
    "    # hka = s\n",
    "    gldm_features=np.concatenate((compute_14_features(gLDM1), compute_14_features(gLDM2),\n",
    "                                  compute_14_features(gLDM3),compute_14_features(gLDM4)), axis=0)\n",
    "    \n",
    "    \n",
    "    glcms =greycomatrix(img, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4])#GLCM in four directions\n",
    "    glcm_features=np.concatenate((compute_14_features(im2double(glcms[:, :, 0, 0])), \n",
    "                                  compute_14_features(im2double(glcms[:, :, 0, 1])),\n",
    "                                  compute_14_features(im2double(im2double(glcms[:, :, 0, 2]))),\n",
    "                                  compute_14_features(glcms[:, :, 0, 3])), axis=0)\n",
    "    \n",
    "    mel_spec = librosa.feature.inverse.mel_to_stft(img_rescaled.astype(np.float))\n",
    "    # Convert to decibels\n",
    "    mel_spec_db = librosa.amplitude_to_db(np.abs(mel_spec), ref=np.max)\n",
    "\n",
    "    # Extract spectral features\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(S=np.abs(mel_spec_db))\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(S=np.abs(mel_spec_db))\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(S=np.abs(mel_spec_db))\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(S=np.abs(mel_spec_db))\n",
    "    \n",
    "    spectral_features=np.concatenate((compute_14_features(spectral_centroid), compute_14_features(spectral_bandwidth),\n",
    "                                  compute_14_features(spectral_contrast),compute_14_features(spectral_rolloff)), axis=0)\n",
    "    # MFCC feature\n",
    "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec_db))\n",
    "    mfcc_feature =  compute_14_features(mfcc) \n",
    "    \n",
    "    feature_vector=np.concatenate((texture_features,fft_features,wavelet_features,gldm_features,glcm_features, spectral_features, mfcc_feature), axis=0).reshape(1,322)#merge to create a feature vector of 252\n",
    "    feature_pool=np.concatenate((feature_pool,feature_vector), axis=0)\n",
    "\n",
    "feature_pool=np.delete(feature_pool, 0, 0)\n",
    "feature_pool=np.concatenate((feature_pool,label*np.ones(len(feature_pool)).reshape(len(feature_pool),1)), axis=1)#add label to the last column   \n",
    "sio.savemat(output_file_name + '_322.mat', {output_file_name: feature_pool})#save the created feature pool as a mat file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14556,
     "status": "ok",
     "timestamp": 1651043585968,
     "user": {
      "displayName": "Kim-Ngoc Thi Le",
      "userId": "07740631987100063646"
     },
     "user_tz": -540
    },
    "id": "zMwulmIZkRIM",
    "outputId": "0036ed45-90a2-4ccd-cd3e-1b6ebb255c3e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# evaluate extracted features on mat files\n",
    "# =============================================================================\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc , classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# =============================================================================\n",
    "# source dir\n",
    "# =============================================================================\n",
    "source_dir='./'\n",
    "# =============================================================================\n",
    "# load mat files\n",
    "# =============================================================================\n",
    "normal_features=sio.loadmat(os.path.join(source_dir,'normal_322.mat')) \n",
    "normal_features=normal_features['normal']\n",
    "\n",
    "crackle_features=sio.loadmat(os.path.join(source_dir,'crackle_322.mat')) \n",
    "crackle_features=crackle_features['crackle']\n",
    "\n",
    "wheeze_features=sio.loadmat(os.path.join(source_dir,'wheeze_322.mat')) \n",
    "wheeze_features=wheeze_features['wheeze']\n",
    "\n",
    "both_features=sio.loadmat(os.path.join(source_dir,'both_322.mat')) \n",
    "both_features=both_features['both']  \n",
    "\n",
    "# pneumonia_features=sio.loadmat(os.path.join(source_dir,'pneumonia.mat')) \n",
    "# pneumonia_features=pneumonia_features['pneumonia']    \n",
    "\n",
    "scores=np.concatenate((normal_features[:,:-1], crackle_features[:,:-1], wheeze_features[:,:-1],both_features[:,:-1]), axis=0)\n",
    "targets=np.concatenate((normal_features[:,-1],crackle_features[:,-1], wheeze_features[:,-1], both_features[:,-1]), axis=0)\n",
    "# =============================================================================\n",
    "# Normalization\n",
    "# =============================================================================\n",
    "min_max_scaler=MinMaxScaler()\n",
    "scores = min_max_scaler.fit_transform(scores) \n",
    "# =============================================================================\n",
    "# correlation map and histogram \n",
    "# =============================================================================\n",
    "df = pd.DataFrame(scores)             \n",
    "corrMatrix = df.corr().abs().fillna(0)\n",
    "fig = plt.figure()\n",
    "sn.heatmap(corrMatrix,xticklabels=50,yticklabels=50,cmap='jet')\n",
    "# plt.savefig('corr_map',dpi=300,format='eps')\n",
    "plt.savefig('cor_Map.jpg',dpi=300)\n",
    "# \n",
    "fig = plt.figure()\n",
    "(h,x)=np.histogram(corrMatrix.to_numpy().reshape(1,322*322), bins=8) \n",
    "# (h, x) = np.histogram(corrMatrix.to_numpy().reshape(1, -1), bins=8)\n",
    "\n",
    "plt.bar(x[1:],h/(322*322),width=0.1)            \n",
    "plt.grid(linewidth=.3)  \n",
    "plt.xlabel('Correlation coefficient value')\n",
    "plt.ylabel('Frequency percentage (%)')\n",
    "plt.savefig('corr_hist.jpg',dpi=300)\n",
    "# =============================================================================\n",
    "# auc value graphs where positive label is normal             \n",
    "# =============================================================================\n",
    "pos_label=0   #normal\n",
    "roc_list=[]  \n",
    "for idx in range(scores.shape[1]):               \n",
    "    fpr, tpr, thresholds = roc_curve(targets, scores[:,idx], pos_label=pos_label)\n",
    "    auc_value=auc(fpr, tpr)\n",
    "    if auc_value<0.5:\n",
    "        auc_value=1-auc_value\n",
    "    roc_list.append(auc_value)# calculate auc value\n",
    "\n",
    "print(np.argmax(roc_list))\n",
    "print(np.max(roc_list))\n",
    "fig = plt.figure()\n",
    "plt.bar([1,2,3,4,5,6,7],[np.mean(roc_list[:14]),np.mean(roc_list[14:28])\n",
    "        ,np.mean(roc_list[28:140]),np.mean(roc_list[140:196]),np.mean(roc_list[196:252]), np.mean(roc_list[252:308]), \n",
    "                         np.mean(roc_list[308:322])] ,width=0.5)\n",
    "\n",
    "plt.xticks([1,2,3,4,5,6,7], ('Texture', 'FFT', 'Wavelet', 'GLDM', 'GLCM', 'Spectral', 'MFCC'))\n",
    "plt.grid(linewidth=.3)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('pos_label=Normal')\n",
    "plt.ylabel('Average AUC value')\n",
    "plt.savefig('avg_bar_pos_normal.jpg',dpi=300)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([i for i in range(1,scores.shape[1]+1)],np.sort(roc_list)[::-1],'x')\n",
    "plt.grid(linewidth=.3)\n",
    "plt.xlim([0.0, scores.shape[1]+1])\n",
    "plt.ylim([0.5, 1])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Sorted AUC values')\n",
    "plt.title('pos_label=Normal')\n",
    "plt.savefig('auc_pos_normal.jpg',dpi=300)\n",
    "\n",
    "# =============================================================================\n",
    "# auc value graphs where positive label is crackle\n",
    "# =============================================================================\n",
    "pos_label=1   # crackle\n",
    "roc_list=[]  \n",
    "for idx in range(scores.shape[1]):               \n",
    "    fpr, tpr, thresholds = roc_curve(targets, scores[:,idx], pos_label=pos_label)\n",
    "    auc_value=auc(fpr, tpr)\n",
    "    if auc_value<0.5:\n",
    "        auc_value=1-auc_value\n",
    "    roc_list.append(auc_value)# calculate auc value\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar([1,2,3,4,5,6,7],[np.mean(roc_list[:14]),np.mean(roc_list[14:28])\n",
    "        ,np.mean(roc_list[28:140]),np.mean(roc_list[140:196]),np.mean(roc_list[196:252]), np.mean(roc_list[252:308]), \n",
    "                         np.mean(roc_list[308:322])] ,width=0.5)\n",
    "\n",
    "plt.xticks([1,2,3,4,5,6,7], ('Texture', 'FFT', 'Wavelet', 'GLDM', 'GLCM', 'Spectral', 'MFCC'))\n",
    "plt.grid(linewidth=.3)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('pos_label=Abnormal')\n",
    "plt.ylabel('Average AUC value')\n",
    "plt.savefig('avg_bar_pos_abnormal.jpg',dpi=300)\n",
    "\n",
    "# =============================================================================\n",
    "# auc value graphs where positive label is wheeze\n",
    "# =============================================================================\n",
    "pos_label=2   # wheeze\n",
    "roc_list=[]  \n",
    "for idx in range(scores.shape[1]):               \n",
    "    fpr, tpr, thresholds = roc_curve(targets, scores[:,idx], pos_label=pos_label)\n",
    "    auc_value=auc(fpr, tpr)\n",
    "    if auc_value<0.5:\n",
    "        auc_value=1-auc_value\n",
    "    roc_list.append(auc_value)# calculate auc value\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar([1,2,3,4,5,6,7],[np.mean(roc_list[:14]),np.mean(roc_list[14:28])\n",
    "        ,np.mean(roc_list[28:140]),np.mean(roc_list[140:196]),np.mean(roc_list[196:252]), np.mean(roc_list[252:308]), \n",
    "                         np.mean(roc_list[308:322])] ,width=0.5)\n",
    "\n",
    "plt.xticks([1,2,3,4,5,6,7], ('Texture', 'FFT', 'Wavelet', 'GLDM', 'GLCM', 'Spectral', 'MFCC'))\n",
    "plt.grid(linewidth=.3)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('pos_label=Abnormal')\n",
    "plt.ylabel('Average AUC value')\n",
    "plt.savefig('avg_bar_pos_abnormal.jpg',dpi=300)\n",
    "\n",
    "# =============================================================================\n",
    "# auc value graphs where positive label is both\n",
    "# =============================================================================\n",
    "pos_label = 3   # both\n",
    "roc_list = []  \n",
    "for idx in range(scores.shape[1]):               \n",
    "    fpr, tpr, thresholds = roc_curve(targets, scores[:,idx], pos_label=pos_label)\n",
    "    auc_value=auc(fpr, tpr)\n",
    "    if auc_value<0.5:\n",
    "        auc_value=1-auc_value\n",
    "    roc_list.append(auc_value)# calculate auc value\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar([1,2,3,4,5,6,7],[np.mean(roc_list[:14]),np.mean(roc_list[14:28])\n",
    "        ,np.mean(roc_list[28:140]),np.mean(roc_list[140:196]),np.mean(roc_list[196:252]), np.mean(roc_list[252:308]), \n",
    "                         np.mean(roc_list[308:322])] ,width=0.5)\n",
    "\n",
    "plt.xticks([1,2,3,4,5,6,7], ('Texture', 'FFT', 'Wavelet', 'GLDM', 'GLCM', 'Spectral', 'MFCC'))\n",
    "plt.grid(linewidth=.3)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('pos_label=Abnormal')\n",
    "plt.ylabel('Average AUC value')\n",
    "plt.savefig('avg_bar_pos_abnormal.jpg',dpi=300)\n",
    "\n",
    "print(np.argmax(roc_list))\n",
    "print(np.max(roc_list))\n",
    "fig = plt.figure()\n",
    "plt.plot([i for i in range(1,scores.shape[1]+1)],np.sort(roc_list)[::-1],'x')\n",
    "plt.grid(linewidth=.3)\n",
    "plt.xlim([0.0, scores.shape[1]+1])\n",
    "plt.ylim([0.5, 1])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Sorted AUC values')\n",
    "plt.title('pos_label=Abnormal')\n",
    "plt.savefig('auc_pos_abnormal.jpg',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1651045513744,
     "user": {
      "displayName": "Kim-Ngoc Thi Le",
      "userId": "07740631987100063646"
     },
     "user_tz": -540
    },
    "id": "T4ytbxdVk_8B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import*\n",
    "# =============================================================================\n",
    "# source_dir\n",
    "# =============================================================================\n",
    "source_dir= './'\n",
    "# =============================================================================\n",
    "# load mat files\n",
    "# =============================================================================\n",
    "normal_features=sio.loadmat(os.path.join(source_dir,'normal_322.mat')) \n",
    "normal_features=normal_features['normal']\n",
    "\n",
    "crackle_features=sio.loadmat(os.path.join(source_dir,'crackle_322.mat')) \n",
    "crackle_features=crackle_features['crackle']\n",
    "\n",
    "wheeze_features=sio.loadmat(os.path.join(source_dir,'wheeze_322.mat')) \n",
    "wheeze_features=wheeze_features['wheeze']\n",
    "\n",
    "both_features=sio.loadmat(os.path.join(source_dir,'both_322.mat')) \n",
    "both_features=both_features['both']    \n",
    "\n",
    "X = np.concatenate((normal_features[:,:-1], crackle_features[:,:-1], wheeze_features[:,:-1],both_features[:,:-1]), axis=0)\n",
    "y = np.concatenate((normal_features[:,-1],crackle_features[:,-1], wheeze_features[:,-1], both_features[:,-1]), axis=0)\n",
    "print(y.shape)\n",
    "# =============================================================================\n",
    "# normalization\n",
    "# =============================================================================\n",
    "min_max_scaler=MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X) \n",
    "# =============================================================================\n",
    "# feature reduction (K-PCA)\n",
    "# =============================================================================\n",
    "transformer = KernelPCA(n_components=97, kernel='linear') #30% of 322 = 97\n",
    "X = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as kl\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.random.set_seed(42)\n",
    "# Define function to build model with specified random seed\n",
    "def build_model(feature_size, n_classes, dropout):\n",
    "    \"\"\" Build a small model for multi-label classification \"\"\"\n",
    "    inp = kl.Input((feature_size,))\n",
    "    x = kl.Dense(1024, activation='relu')(inp)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Dense(640, activation='relu')(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Dense(512, activation='relu')(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Dense(256, activation='relu')(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Dense(128, activation='relu')(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Dense(64, activation='relu')(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    x = kl.Dense(32, activation='relu')(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Dropout(dropout)(x)\n",
    "    out = kl.Dense(n_classes, activation='softmax')(x) # change softmax\n",
    "    model = keras.Model(inputs=inp, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# devide data into test,train, and validation sets\n",
    "# =============================================================================\n",
    "y = to_categorical(y)\n",
    "print('y value: ', y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=1)\n",
    "print('X train - y train:', X_train.shape, y_train.shape)\n",
    "print('X test - y test:', X_test.shape, y_test.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=1)# 0.25\n",
    "print('X train - val', X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 15310,
     "status": "ok",
     "timestamp": 1651046280788,
     "user": {
      "displayName": "Kim-Ngoc Thi Le",
      "userId": "07740631987100063646"
     },
     "user_tz": -540
    },
    "id": "i9gnbx24-UHm",
    "outputId": "82757d69-c510-4b87-cd02-78cecf6c2aac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# build model\n",
    "# =============================================================================\n",
    "model = build_model(feature_size=X_train.shape[-1], n_classes=4, dropout= 0.2)\n",
    "print('Built model!!!', model)\n",
    "# # =============================================================================\n",
    "# train model\n",
    "# =============================================================================\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "opt = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "criterion = tf.keras.losses.categorical_crossentropy\n",
    "model.compile(optimizer=opt, loss=criterion,metrics=['acc'])\n",
    "trainedmodel = model.fit(X_train, y_train,batch_size = 128,epochs=100, validation_data = (X_val, y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(model.history.history['val_loss'], 'r',model.history.history['loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Score')\n",
    "plt.grid(1)\n",
    "plt.savefig('training_loss.jpg',dpi=300)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "accuracy = trainedmodel.history['acc']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(1)\n",
    "plt.savefig('Acc.jpg',dpi=300)\n",
    "# =============================================================================\n",
    "# evaluate model\n",
    "# =============================================================================\n",
    "print('x test:', X_test.shape)\n",
    "Y_Score=model.predict(X_test)\n",
    "print('Y_score:', Y_Score.shape)\n",
    "y_pred = np.argmax(Y_Score, axis=1)\n",
    "print('y test: ', y_test.shape)\n",
    "cm=confusion_matrix(np.argmax(y_test, axis=1),y_pred)\n",
    "print(cm)\n",
    "\n",
    "fig = plt.figure()\n",
    "plot_confusion_matrix(cm,classes=['normal', 'abnormal'])\n",
    "plt.savefig('conf_matrix.jpg',dpi=300)\n",
    "\n",
    "test_loss=model.evaluate(X_test,y_test,verbose=1)#evaluate model\n",
    "print(test_loss)#print test loss and metrics information\n",
    "\n",
    "# =============================================================================\n",
    "# ROC curve where positive label is 0 \n",
    "# =============================================================================\n",
    "pos_label=0\n",
    "fpr, tpr, thresholds = roc_curve(np.argmax(y_test, axis=1), Y_Score[:,pos_label], pos_label=pos_label)\n",
    "roc_auc = auc(fpr, tpr)# calculate auc value\n",
    "fig = plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)\\npos_label=Normal' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.grid(1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_Normal.jpg',dpi=300)\n",
    "\n",
    "# =============================================================================\n",
    "# ROC curve where positive label is normal\n",
    "# =============================================================================\n",
    "pos_label=1\n",
    "fpr, tpr, thresholds = roc_curve(np.argmax(y_test, axis=1), Y_Score[:,pos_label], pos_label=pos_label)\n",
    "roc_auc = auc(fpr, tpr)# calculate auc value\n",
    "fig = plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)\\npos_label=Abnormal' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.grid(1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_abnormal.jpg',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_cm = confusion_matrix(np.argmax(y_test, axis=1),y_pred)\n",
    "\n",
    "se = (avg_cm[1][1] + avg_cm[2][2] + avg_cm[3][3])/(avg_cm[0][1] + avg_cm[1][1] + avg_cm[2][1] + avg_cm[3][1] \n",
    "                                                   + avg_cm[0][2] + avg_cm[1][2] + avg_cm[2][2] + avg_cm[3][2]\n",
    "                                                  + avg_cm[0][3] + avg_cm[1][3] + avg_cm[2][3] + avg_cm[3][3])\n",
    "sp = avg_cm[0][0]/(avg_cm[0][0] + avg_cm[1][0] + avg_cm[2][0] + avg_cm[3][0])\n",
    "sc = (se+sp)/2\n",
    "print('Specificity Sp:', sp)\n",
    "print('Sensitivity Se:', se)\n",
    "print('Score Sc:', sc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2XDhsWEzclOmJ8gvQLOEi",
   "name": "Covid_3gr.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
